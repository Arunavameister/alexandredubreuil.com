<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Music Generation with Magenta: Using Machine Learning in Arts</title>
  <link rel="stylesheet"
        href="../common/bower_components/reveal.js/css/reveal.css">
  <link rel="stylesheet"
        href="../common/bower_components/reveal.js/css/theme/white.css">
  <link rel="stylesheet"
        href="../common/css/magenta-theme.css">
  <script
      src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js?skin=desert"></script>
</head>
<body>


<footer>
  <img class="logo"
       src="../common/img/magenta-logo-01.png"
       alt="Magenta logo">
  <div class="title">
    <div>Music Generation with Magenta</div>
    <div class="subtitle">Using Machine Learning in Arts</div>
  </div>
  <div class="title right">
    <div>Alexandre DuBreuil</div>
    <div class="subtitle">alexandredubreuil.com</div>
  </div>
</footer>

<div class="reveal">

  <div class="slides">

    <section>

      <section>
        <h1 class="align-left">
          Music Generation with <strong style="color:#4c1130ff">Magenta</strong>
        </h1>
        <h2 class="align-right">Using Machine Learning in Arts</h2>
        <h3 class="align-right">Alexandre DuBreuil</h3>
        <h4 class="align-right">@dubreuia</h4>
      </section>

      <section>
        <h3>Alexandre DuBreuil</h3>
        <p>
          Software engineer, sound designer, conference speaker and open source
          maintainer.
        </p>
        <p>@dubreuia</p>
      </section>

    </section>

    <section>

      <section>
        <h3>Generative Music</h3>
        <p>(in 5 minutes)</p>
      </section>

      <section data-background="../common/img/magenta/ableton-live-02.png"
               class="background">
        <h5>
          Make music without being a musician
        </h5>
        <p>
          Maybe you don't know how to improvise, maybe you need help to
          compose and that's okay.
        </p>
        <span class="figure-caption"><a
            href="https://media.pitbullaudio.com/media/catalog/product/cache/1/image/9df78eab33525d08d6e5fb8d27136e95/s/u/suite_3_1.jpeg">
          https://media.pitbullaudio.com/media/catalog/product/cache/1/image/9df78eab33525d08d6e5fb8d27136e95/s/u/suite_3_1.jpeg
        </a></span>
      </section>

      <section data-background="../common/img/magenta/ableton-live-02.png"
               class="background">
        <h5>
          "Why you should build silly things" - Monica Dinculescu
        </h5>
        <p>
          It is okay to make mistakes, and generative systems makes
          tons of them (also humans). It doesn't always sound good.
        </p>
        <p>
          The weird and the strange is good (Brian Eno)
        </p>
        <span class="figure-caption"><a
            href="https://media.pitbullaudio.com/media/catalog/product/cache/1/image/9df78eab33525d08d6e5fb8d27136e95/s/u/suite_3_1.jpeg">
          https://media.pitbullaudio.com/media/catalog/product/cache/1/image/9df78eab33525d08d6e5fb8d27136e95/s/u/suite_3_1.jpeg
        </a></span>
      </section>

      <section data-background="../common/img/magenta/ableton-live-02.png"
               class="background">
        <h5>
          Helping people build generative systems
        </h5>
        <p>
          Art exhibit, generative radio, interactive art.
        </p>
        <span class="figure-caption"><a
            href="https://media.pitbullaudio.com/media/catalog/product/cache/1/image/9df78eab33525d08d6e5fb8d27136e95/s/u/suite_3_1.jpeg">
          https://media.pitbullaudio.com/media/catalog/product/cache/1/image/9df78eab33525d08d6e5fb8d27136e95/s/u/suite_3_1.jpeg
        </a></span>
      </section>

      <!--
      <section>
        <p>TODO remove (or compress to 1 slide)</p>
        <h4>Example: Musikalisches WÃ¼rfelspiel</h4>
        <p>
          Back in the 18th century, a "musical dice game" was popular, where
          the result of a dice throw would choose a segment of a predefined
          score, creating a score when repeated.
        </p>
        <p>
          <strong style="color:darkmagenta">Random</strong> <strong>generative
          art:</strong> the outcome of the generated art is partially or
          completely defined by the chance's outcome
        </p>
        <span class="figure-caption"><a
            href="https://en.wikipedia.org/wiki/Musikalisches_W%C3%BCrfelspiel">
          en.wikipedia.org/wiki/Musikalisches_W%C3%BCrfelspiel
        </a></span>
      </section>

      <section>
        <h4>Example: Algorave</h4>
        <p>
          An Algorave is an event where people dance to music generated from
          algorithms, often using live coding techniques, and short for
          "algorithmic rave".
        </p>
        <p>
          <strong style="color:darkmagenta;">Rule based / algorithmic</strong>
          <strong>generative art: </strong> the result of such generation is
          deterministic and defined by a set of rules.
        </p>
        <span class="figure-caption"><a
            href="https://en.wikipedia.org/wiki/Algorave">
          en.wikipedia.org/wiki/Algorave
        </a></span>
      </section>

      <section>
        <h4>Example: Illiac Suite</h4>
        <p>
          The Illiac Suite is one of the first score composed by an electronic
          computer. In one of the fourth movement, Markov Chains are used.
        </p>
        <p>
          <strong style="color:darkmagenta;">Stochastic</strong>
          <strong>generative art: </strong> the artwork generation is
          probabilistic.
        </p>
        <span class="figure-caption"><a
            href="https://en.wikipedia.org/wiki/Illiac_Suite">
          en.wikipedia.org/wiki/Illiac_Suite
        </a></span>
      </section>
      -->

      <section>
        <h4>What?</h4>
        <p>
          "Generative art is an artwork partially or completely
          created by an autonomous system."
        </p>
      </section>

      <section>
        <h4>And Machine Learning?</h4>
        <p>
          Hand crafting the rules of a painting or the rules of a music style
          might be a hard task. That's why Machine Learning is so interesting
          in arts: it can learn complex functions.
        </p>
      </section>

      <section class="align-left">
        <h4>Music generation with RNNs</h4>
        <p>
          <img width="250"
               class="figure"
               src="../common/img/magenta/rnn.png"
               alt="RNN diagram"/>
          Recurrent Neural Networks (RNNs) solves two important problems for
          music generation: <strong>operating on sequences for the inputs
          and outputs</strong> and <strong>keeping an internal state of
          past events</strong>.
        </p>
        <span class="figure-caption">
          <a href="https://www.asimovinstitute.org/wp-content/uploads/2016/09/rnn.png">
            www.asimovinstitute.org/wp-content/uploads/2016/09/rnn.png</a>
        </span>
      </section>

      <section class="align-left">
        <h4>Long-term structure with LSTMs</h4>
        <p>
          <img width="250"
               class="figure"
               src="../common/img/magenta/lstm.png"
               alt="LSTM diagram"/>
          Most RNN uses Long Short-Term Memory (LSTM) cells, because by
          themselves, RNNs are hard to train because of the problems of
          vanishing and exploding gradient, making long-term dependencies
          hard to learn.
        </p>
        <p>
          By using input, output and forget gates in the cell, LSTMs
          can learn mechanisms to keep or forget information as they go.
        </p>
        <span class="figure-caption">
          <a href="https://www.asimovinstitute.org/wp-content/uploads/2016/09/lstm.png">
            https://www.asimovinstitute.org/wp-content/uploads/2016/09/lstm.png
          </a>
        </span>
      </section>

      <section class="align-left">
        <h4>Latent space interpolation with VAEs</h4>
        <p>
          <img width="250"
               class="figure"
               src="../common/img/magenta/vae.png"
               alt="VAE diagram"/>
          Variational Autoencoders (VAEs) are a pair of networks where an
          encoder reduces the input to a lower dimentionality (<strong>latent
          space</strong>), from which a decoder tries to reproduce the
          input.
        </p>
        <p>
          The latent space is continuous and follows a probability
          distribution, meaning it is possible to sample from it.
          VAEs are inherently generative models: they can
          <strong>sample</strong> and <strong>interpolate</strong> (move in
          the latent space) between two points.
        </p>
        <span class="figure-caption">
          <a href="https://www.asimovinstitute.org/wp-content/uploads/2016/09/vae.png">
            https://www.asimovinstitute.org/wp-content/uploads/2016/09/vae.png
          </a>
        </span>
      </section>

      <section class="align-left">
        <h4>Audio generation with WaveNet Autoencoders</h4>
        <p>
          <img width="500"
               class="figure"
               src="../common/img/magenta/wavenet.png"
               alt="Wavenet diagram"/>
          WaveNet is a convolutional neural network (CNN) taking raw signal
          as an input and synthesizing output audio sample by sample. The
          WaveNet Autoencoder present in Magenta is a AE network capable of
          learning its own temporal embedding, resulting in a latent space
          from which is it possible to <strong>sample</strong> and
          <strong>mix</strong> elements.
        </p>
        <span class="figure-caption">
          <a href="https://magenta.tensorflow.org/assets/nsynth_05_18_17/encoder-decoder.png">
            magenta.tensorflow.org/assets/nsynth_05_18_17/encoder-decoder.png
          </a>
        </span>
      </section>

      <section>
        <h4>Representation: MIDI</h4>
        <p>
          More interesting to work with MIDI because it shows the underlying
          structure of music, but doesn't define the actual sound, you need
          to use instruments (numeric or analogic).
        </p>
        <img width=""
             class="figure"
             src="../common/img/magenta/midi.png"
             alt="MIDI diagram"/>
      </section>

      <section>
        <h4>Representation: audio</h4>
        <p>
          Working with audio is harder more direct: the generation doesn't
          need the extra step of adding instruments.
        </p>
        <img
            class="figure"
            src="../common/img/magenta/spectrogram.png"
            alt="Audio diagram"/>
        <p></p>
        <span class="figure-caption">
          <a href="https://upload.wikimedia.org/wikipedia/commons/c/c5/Spectrogram-19thC.png">
            https://upload.wikimedia.org/wikipedia/commons/c/c5/Spectrogram-19thC.png
          </a>
        </span>
      </section>

      <section>
        <h4>What's in the box?</h4>
        <p>Interesting networks from Magenta for music generation:</p>
        <table class="smaller">
          <thead>
          <tr>
            <th>Model</th>
            <th>Network</th>
            <th>Repr.</th>
            <th>Encoding</th>
          </tr>
          </thead>
          <tbody>
          <tr>
            <td>DrumsRNN</td>
            <td>LSTM</td>
            <td>MIDI</td>
            <td>polyphonic-ish</td>
          </tr>
          <tr>
            <td>MelodyRNN</td>
            <td>LSTM</td>
            <td>MIDI</td>
            <td>monophonic</td>
          </tr>
          <tr>
            <td>PolyphonyRNN</td>
            <td>LSTM</td>
            <td>MIDI</td>
            <td>polyphonic</td>
          </tr>
          <tr>
            <td>PerformanceRNN</td>
            <td>LSTM</td>
            <td>MIDI</td>
            <td>polyphonic, expressive timing</td>
          </tr>
          <tr>
            <td>MusicVAE</td>
            <td>VAE</td>
            <td>MIDI</td>
            <td>multiple</td>
          </tr>
          <tr>
            <td>NSynth</td>
            <td>Wavenet</td>
            <td>Audio</td>
            <td>-</td>
          </tr>
          </tbody>
        </table>
      </section>

    </section>

    <section>

      <section>
        <h3>Live code: Generate a track</h3>
        <p>(in 15 minutes)</p>
      </section>

      <section data-background="../common/img/magenta/ableton-live-02.png"
               class="background">
        <h5>
          Step 1: make everything sound like a cat.
        </h5>
        <p>
          We'll use NSynth to mix cat sounds with other sounds.
        </p>
        <span class="figure-caption"><a
            href="https://media.pitbullaudio.com/media/catalog/product/cache/1/image/9df78eab33525d08d6e5fb8d27136e95/s/u/suite_3_1.jpeg">
          https://media.pitbullaudio.com/media/catalog/product/cache/1/image/9df78eab33525d08d6e5fb8d27136e95/s/u/suite_3_1.jpeg
        </a></span>
      </section>

      <section>
        <h4>Step 1: The sounds</h4>
        <p>
          <audio controls>
            <source src="code/sounds/83249__zgump__bass-0205__crop.wav"
                    type="audio/wav"/>
          </audio>
          <audio controls>
            <source
                src="code/sounds/160045__jorickhoofd__metal-hit-with-metal-bar-resonance__crop.wav"
                type="audio/wav"/>
          </audio>
          <audio controls>
            <source src="code/sounds/412017__skymary__cat-meow-short__crop.wav"
                    type="audio/wav"/>
          </audio>
          <audio controls>
            <source src="code/sounds/427567__maria-mannone__flute__crop.wav"
                    type="audio/wav"/>
          </audio>
        </p>
      </section>

      <section>
        <h4>Step 1: Sound generation - NSynth</h4>
        <p>See "code/nsynth.py" and method app in this repo.</p>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-python" data-trim data-noescape>
def app(unused_argv):
  encoding_mix = mix(FLAGS.wav1,
                     FLAGS.wav2,
                     FLAGS.sample_length,
                     checkpoint=FLAGS.checkpoint)
  date_and_time = time.strftime("%Y-%m-%d_%H%M%S")
  output = os.path.join("output", "synth",
                        f"{date_and_time}.wav")
  fastgen.synthesize(encoding_mix,
                     checkpoint_path=FLAGS.checkpoint,
                     save_paths=[output])
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>Step 1: Sound generation - NSynth</h4>
        <p>See "code/nsynth.py" and method mix in this repo.</p>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-python" data-trim data-noescape>
def mix(wav1: str,
        wav2: str,
        sample_length: int = None,
        sample_rate: int = 16000,
        checkpoint=None):
  encoding1 = encode(wav1, sample_length,
                     sample_rate, checkpoint)
  encoding2 = encode(wav2, sample_length,
                     sample_rate, checkpoint)
  encoding_mix = (encoding1 + encoding2) / 2.0
  return encoding_mix
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>Step 1: Sound generation - NSynth</h4>
        <p>See "code/nsynth.py" and method encode in this repo.</p>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-python" data-trim data-noescape>
def encode(wav: str,
           sample_length: int = None,
           sample_rate: int = 16000,
           checkpoint=None):
  audio = utils.load_audio(wav,
                           sample_length=sample_length,
                           sr=sample_rate)
  encoding = fastgen.encode(audio, checkpoint, sample_length)
  return encoding
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>Step 1: TODO gansynth</h4>
      </section>

      <section data-background="../common/img/magenta/ableton-live-02.png"
               class="background">
        <h5>
          Step 2: sequence the cats
        </h5>
        <p>
          We'll use DrumsRNN and MelodyRNN to generate MIDI to play the
          samples.
        </p>
        <span class="figure-caption"><a
            href="https://media.pitbullaudio.com/media/catalog/product/cache/1/image/9df78eab33525d08d6e5fb8d27136e95/s/u/suite_3_1.jpeg">
          https://media.pitbullaudio.com/media/catalog/product/cache/1/image/9df78eab33525d08d6e5fb8d27136e95/s/u/suite_3_1.jpeg
        </a></span>
      </section>

      <section>
        <h4>Step 2: Score generation - DrumsRNN + MelodyRNN</h4>
        <p>
          See "code/sequences.py" and method reset in this repo.
        </p>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-python" data-trim data-noescape>
def reset(loop_start_time: float,
          loop_end_time: float,
          seconds_per_loop: float):
  sequence = music_pb2.NoteSequence()
  sequence = loop(sequence,
                  loop_start_time,
                  loop_end_time,
                  seconds_per_loop)
  return sequence
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>Step 2: Score generation - DrumsRNN + MelodyRNN</h4>
        <p>
          See "code/sequences.py" and method loop in this repo.
        </p>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-python" data-trim data-noescape>
def loop(sequence: NoteSequence,
         loop_start_time: float,
         loop_end_time: float,
         seconds_per_loop: float):
  sequence = ss.trim_note_sequence(sequence,
                                   loop_start_time,
                                   loop_end_time)
  sequence = ss.shift_sequence_times(sequence,
                                     seconds_per_loop)
  return sequence
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>Step 2: Score generation - DrumsRNN + MelodyRNN</h4>
        <p>
          See "code/sequences.py" and method generate in this repo.
        </p>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-python" data-trim data-noescape>
def generate(sequence: NoteSequence,
             name: str,
             bundle_filename: str,
             config_name: str,
             generation_start_time: float,
             generation_end_time: float):
  generator_options = generator_pb2.GeneratorOptions()
  generator_options.args['temperature'].float_value = 1
  generator_options.generate_sections.add(
    start_time=generation_start_time,
    end_time=generation_end_time)
  sequence_generator = get_sequence_generator(name,
                                              bundle_filename,
                                              config_name)
  sequence = sequence_generator.generate(sequence,
                                         generator_options)
  sequence = ss.trim_note_sequence(sequence,
                                   generation_start_time,
                                   generation_end_time)
  return sequence
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>Step 2: Score generation - DrumsRNN + MelodyRNN</h4>
        <p>
          See "code/sequences.py" and method get_sequence_generator
          in this repo.
        </p>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-python" data-trim data-noescape>
def get_sequence_generator(name: str,
                           bundle_filename: str,
                           config_name: str):
  if name == "drums":
    generator = drums_rnn_sequence_generator
  elif name == "melody":
    generator = melody_rnn_sequence_generator
  else:
    raise Exception(f"Unknown sequence generator {name}")

  mm.notebook_utils.download_bundle(bundle_filename, "bundles")
  bundle = mm.sequence_generator_bundle.read_bundle_file(
    os.path.join("bundles", bundle_filename))

  generator_map = generator.get_generator_map()
  sequence_generator = generator_map[config_name](
    checkpoint=None, bundle=bundle)
  sequence_generator.initialize()

  return sequence_generator
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>Let's take a step back: why have we done this app?</h4>
        <p>
          Because it enabled us to explore music together, it serves as a
          creation tool. Was it perfect? No, we had little happy accidents.
        </p>
        <img width="250"
               class="figure"
               src="../common/img/magenta/rnn.png"
               alt="RNN diagram"/>
      </section>

    </section>

    <section>

      <section>
        <h3>Interaction with the outside world</h3>
        <p>(in 5 minutes)</p>
      </section>

      <section>
        <h4>Python to everything using MIDI</h4>
        <p>
          Magenta can send MIDI, which is understood by basically everything
          that makes sound: DAWs (like Ableton Live), software synthesizers
          (like fluidsynth), hardware synthesizers (though USB or MIDI cable),
          etc.
        </p>
      </section>

      <section>
        <h4>Magenta in the browser with Magenta.js (1/2)</h4>
        <p>
          https://tensorflow.github.io/magenta-js/music/index.html
          TODO explain
        </p>
      </section>

      <section>
        <h4>Magenta in the browser with Magenta.js</h4>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-html" data-trim data-noescape>
&lt;html&gt;
&lt;head&gt;
  ...
  &lt;!-- You need to bring your own Tone.js for the player, and tfjs for the model --&gt;
  &lt;script src="https://cdnjs.cloudflare.com/ajax/libs/tone/13.8.21/Tone.js"&gt;&lt;/script&gt;
  &lt;script src="https://cdnjs.cloudflare.com/ajax/libs/tensorflow/1.2.8/tf.min.js"&gt;&lt;/script&gt;
  &lt;!-- Core library, since we're going to use a player --&gt;
  &lt;script src="https://cdn.jsdelivr.net/npm/@magenta/music@^1.0.0/es6/core.js"&gt;&lt;/script&gt;
  &lt;!--Model we want to use --&gt;
  &lt;script src="https://cdn.jsdelivr.net/npm/@magenta/music@^1.0.0/es6/music_vae.js"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;script&gt;
  // Each bundle exports a global object with the name of the bundle.
  const player = new core.Player();
  //...
  const mvae = new music_vae.MusicVAE('https://storage.googleapis.com/magentadata/js/checkpoints/music_vae/mel_2bar_small');
  mvae.initialize().then(() =&gt; {
    //...
  });
&lt;/script&gt;
&lt;/html&gt;
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>Magenta in your DAW with Magenta Studio</h4>
        <p>
          <video autoplay="" loop="" muted="" playsinline="">
            <source src="../common/video/hero.mp4" type="video/mp4">
          </video>
          <span class="figure-caption">https://magenta.tensorflow.org/studio/assets/studio/hero.mp4</span>
        </p>
      </section>

    </section>

    <section>

      <section>

        <h3>Training</h3>
        <p>(in 5 minutes)</p>
      </section>

      <section>
        <h4>Dataset</h4>
        <ul>
          <li>TODO lakhs dataset https://colinraffel.com/projects/lmd/</li>
          <li>TODO msd dataset http://millionsongdataset.com/</li>
          <li>TODO nsynth</li>
          <li>TODO custom</li>
        </ul>
      </section>

      <section>
        <h4>Tensorboard / tensorflow</h4>
        <ul>
          <li>TODO</li>
        </ul>
      </section>

    </section>

    <section>

      <section>
        <h3>Closing</h3>
      </section>

      <section>
        <h4>Dreambank</h4>
        <ul>
          <li>TODO</li>
        </ul>
      </section>

      <section>
        <h4>Book</h4>
        <ul>
          <li>TODO</li>
        </ul>
      </section>

    </section>

  </div>
</div>
<script src="../common/bower_components/reveal.js/js/reveal.js"></script>
<!-- TODO add jquery -->
<script src="../common/js/reveal.js"></script>
<script>
    Reveal.initialize({
        progress: true,
        history: true,
    });
</script>
</body>
</html>
